{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monetary-darkness",
   "metadata": {},
   "source": [
    "# Data/Package Import\n",
    "Here the validation dataset is used in place of the whole dataset. When running on DIAS servers, we'll start with Tadhg's pipeline as if we were taking in everything from scratch. There will be 4 outputs from the script: A parquet file containing all the processed data and 3 parquet files ready for training, testing and validating our RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collective-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import datetime\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.dates import DateFormatter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agricultural-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(51225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "isolated-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path(\"../data\")\n",
    "crossing_list_csv = Path(\"mercury_boundary_crossing_list.csv\")\n",
    "validation_list_file = Path(\"validation_list.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accompanied-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Messenger Data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crossing_list = pd.read_csv(\n",
    "    data_directory / crossing_list_csv,\n",
    "    sep=',', skipinitialspace=True, index_col=False,\n",
    "    usecols=[\"Start_Date\", \"End_Date\"],\n",
    "    parse_dates=[\"Start_Date\", \"End_Date\"], infer_datetime_format=True\n",
    ")\n",
    "\n",
    "# Converting transitions and Messenger data to datetime format\n",
    "stopgap_minutes_delta = datetime.timedelta(minutes=10)\n",
    "\n",
    "crossing_list[\"Start_Date\"] -= stopgap_minutes_delta\n",
    "crossing_list[\"End_Date\"] += stopgap_minutes_delta\n",
    "\n",
    "print(\"Importing Messenger Data...\")\n",
    "\n",
    "df = pd.read_parquet(data_directory / validation_list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moral-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>End_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-03-24 01:00:01</td>\n",
       "      <td>2011-03-24 01:23:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-03-24 13:12:55</td>\n",
       "      <td>2011-03-24 13:33:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-03-25 01:09:05</td>\n",
       "      <td>2011-03-25 01:31:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-25 13:17:39</td>\n",
       "      <td>2011-03-25 13:40:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-03-26 01:22:39</td>\n",
       "      <td>2011-03-26 01:45:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11386</th>\n",
       "      <td>2014-03-16 12:08:00</td>\n",
       "      <td>2014-03-16 13:37:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11387</th>\n",
       "      <td>2014-03-16 20:23:07</td>\n",
       "      <td>2014-03-16 21:05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11388</th>\n",
       "      <td>2014-03-17 03:51:22</td>\n",
       "      <td>2014-03-17 04:34:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11389</th>\n",
       "      <td>2014-03-17 12:37:44</td>\n",
       "      <td>2014-03-17 13:45:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11390</th>\n",
       "      <td>2014-03-17 20:08:45</td>\n",
       "      <td>2014-03-17 20:47:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11391 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Start_Date            End_Date\n",
       "0     2011-03-24 01:00:01 2011-03-24 01:23:35\n",
       "1     2011-03-24 13:12:55 2011-03-24 13:33:17\n",
       "2     2011-03-25 01:09:05 2011-03-25 01:31:49\n",
       "3     2011-03-25 13:17:39 2011-03-25 13:40:03\n",
       "4     2011-03-26 01:22:39 2011-03-26 01:45:29\n",
       "...                   ...                 ...\n",
       "11386 2014-03-16 12:08:00 2014-03-16 13:37:28\n",
       "11387 2014-03-16 20:23:07 2014-03-16 21:05:03\n",
       "11388 2014-03-17 03:51:22 2014-03-17 04:34:46\n",
       "11389 2014-03-17 12:37:44 2014-03-17 13:45:04\n",
       "11390 2014-03-17 20:08:45 2014-03-17 20:47:26\n",
       "\n",
       "[11391 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smaller-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = RobustScaler().fit(df[\"B_X_MSO\"].to_numpy().reshape(-1, 1))\n",
    "df[\"B_X_MSO\"] = transformer.transform(df[\"B_X_MSO\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "transformer = RobustScaler().fit(df[\"B_Y_MSO\"].to_numpy().reshape(-1, 1))\n",
    "df[\"B_Y_MSO\"] = transformer.transform(df[\"B_Y_MSO\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "transformer = RobustScaler().fit(df[\"B_Z_MSO\"].to_numpy().reshape(-1, 1))\n",
    "df[\"B_Z_MSO\"] = transformer.transform(df[\"B_Z_MSO\"].to_numpy().reshape(-1, 1))\n",
    "df = df[df.index < \"March 2014\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceramic-decade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_date = datetime.date(2011,6,1)\\nstrdate = datetime.datetime(2012,6,9,0,0,0)\\nenddate = datetime.datetime(2012,6,13,0,0,0)\\n\\n#possible issues between 01/06/12-15/06/12\\n\\nFilename = \"Messenger_\"+strdate.strftime(\"%Y%j\")+\".csv\"\\nCNames = [\\'Year\\', \\'DOY\\', \\'Hour\\', \\'Minute\\', \\'Second\\', \\'X_MSO\\', \\'Y_MSO\\', \\'Z_MSO\\', \\'B_X_MSO\\', \\'B_Y_MSO\\', \\'B_Z_MSO\\']\\n\\ndf1 = pd.read_csv(dir_path + \"/MESSENGER/\"+Filename, sep = \\' \\', skipinitialspace = True, header = None, names = CNames)\\n\\n\\n#====Data assimilation loop\\n\\nn_days = ((enddate -strdate).days)\\n\\nfor day in range(1,n_days):\\n    next_day = strdate+datetime.timedelta(days = day)\\n    \\n    next_day_str = next_day.strftime(\"%Y\")+next_day.strftime(\"%j\")\\n    \\n    Filename = \"Messenger_\"+next_day_str+\".csv\"\\n    df2 = pd.read_csv(dir_path + \"/MESSENGER/\"+Filename, sep = \\' \\', skipinitialspace = True, header = None, names = CNames)\\n    \\n    df = pd.concat([Data,Data2], ignore_index = True)\\n\\n#====end loop\\ndel n_days\\ndel next_day\\ndel next_day_str\\ndel Data2\\n\\ndf[\"Year\"] = df[\"Year\"].astype(str).str.zfill(4)\\ndf[\"DOY\"] = df[\"DOY\"].astype(str).str.zfill(3)\\ndf[\"Hour\"] = df[\"Hour\"].astype(str).str.zfill(2)\\ndf[\"Minute\"] = df[\"Minute\"].astype(str).str.zfill(2)\\ndf[\"Second\"] = pd.to_numeric(Data[\"Second\"], errors = \\'coerce\\').fillna(0).astype(int).astype(str).str.zfill(2)\\n\\n\\ndf[\"Date\"] = df[\"Year\"] + df[\"DOY\"] +df[\"Hour\"] +df[\"Minute\"] +df[\"Second\"]\\n\\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"],format = \"%Y%j%H%M%S\", errors = \\'coerce\\')\\n\\nprint(\"Datetimes Created\")\\n\\ndf.dropna(subset = [\"Date\"], inplace=True)\\n\\ndf = df.set_index(\"Date\")\\ndf = df.resample(\"1S\").mean().interpolate()\\n\\np = np.degrees(np.arctan2(Data[\"Y_MSO\"], df[\"X_MSO\"]))+180.\\ndp = np.array(p[1:])- np.array(p[:-1])\\norbitStartIndex = np.where(dp  < -180)[0]\\nlenp = len(p)\\n\\ndel p\\ndel dp\\n    \\nloc = np.zeros(df.shape[0]).astype(int)\\n\\nxxx = (crosssort[\"S_Date\"] - df.index[0])\\nxxx = xxx.dt.total_seconds()\\n\\nprint(\"Created Location Array\")\\n\\nif np.nanmin(xxx) < 0:\\n    i = np.argmax(np.array(xxx.iloc[np.where(xxx < 0)]))\\n    if crosssort[\"Type\"].iloc[i] == \"BSI\":\\n        loc[:] = 1\\n    \\n    if crosssort[\"Type\"].iloc[i] == \"MPO\":\\n        loc[:] = 1\\n    \\n    if crosssort[\"Type\"].iloc[i] == \"MPI\":\\n        loc[:] = 0\\n    \\n    if crosssort[\"Type\"].iloc[i] == \"BSO\":\\n        loc[:] = 2\\n\\nelse:\\n    loc[:] = 2\\n\\ndel xxx\\n\\nfor i in range(len(crosssort)):\\n    index = np.where((Data.index - crosssort[\"S_Date\"].iloc[i]).total_seconds() == 0)[0]\\n    if len(index) == 1:\\n        index = index[0]\\n        if crosssort[\"Type\"].iloc[i] == \"BSI\":\\n            loc[index:] = -2\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"MPO\":\\n            loc[index:] = -1\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"MPI\":\\n            loc[index:] = -1\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"BSO\":\\n            loc[index:] = -2\\n\\n    index = np.where((Data.index - crosssort[\"E_Date\"].iloc[i]).total_seconds() == 0)[0]\\n    if len(index) == 1:\\n        index = index[0]\\n        if crosssort[\"Type\"].iloc[i] == \"BSI\":\\n            loc[index:] = 1\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"MPO\":\\n            loc[index:] = 1\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"MPI\":\\n            loc[index:] = 0\\n            \\n        if crosssort[\"Type\"].iloc[i] == \"BSO\":\\n            loc[index:] = 2\\n\\ndel crosssort\\nprint(\"Filled Location Array\")\\ndf[\"Location\"] = loc.tolist()\\n\\norbit = np.zeros(lenp)\\nfor i in range(len(orbitStartIndex)):\\n    orbit[orbitStartIndex[i]:] = i\\n\\ndel lenp\\ndel orbitStartIndex\\n\\ndf[\"OrbitNo\"] = orbit.tolist()\\nprint(\"Filled Orbit List\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tadhg's preprocessing stuff, not needed for Jupyter \n",
    "\"\"\"start_date = datetime.date(2011,6,1)\n",
    "strdate = datetime.datetime(2012,6,9,0,0,0)\n",
    "enddate = datetime.datetime(2012,6,13,0,0,0)\n",
    "\n",
    "#possible issues between 01/06/12-15/06/12\n",
    "\n",
    "Filename = \"Messenger_\"+strdate.strftime(\"%Y%j\")+\".csv\"\n",
    "CNames = ['Year', 'DOY', 'Hour', 'Minute', 'Second', 'X_MSO', 'Y_MSO', 'Z_MSO', 'B_X_MSO', 'B_Y_MSO', 'B_Z_MSO']\n",
    "\n",
    "df1 = pd.read_csv(dir_path + \"/MESSENGER/\"+Filename, sep = ' ', skipinitialspace = True, header = None, names = CNames)\n",
    "\n",
    "\n",
    "#====Data assimilation loop\n",
    "\n",
    "n_days = ((enddate -strdate).days)\n",
    "\n",
    "for day in range(1,n_days):\n",
    "    next_day = strdate+datetime.timedelta(days = day)\n",
    "    \n",
    "    next_day_str = next_day.strftime(\"%Y\")+next_day.strftime(\"%j\")\n",
    "    \n",
    "    Filename = \"Messenger_\"+next_day_str+\".csv\"\n",
    "    df2 = pd.read_csv(dir_path + \"/MESSENGER/\"+Filename, sep = ' ', skipinitialspace = True, header = None, names = CNames)\n",
    "    \n",
    "    df = pd.concat([Data,Data2], ignore_index = True)\n",
    "\n",
    "#====end loop\n",
    "del n_days\n",
    "del next_day\n",
    "del next_day_str\n",
    "del Data2\n",
    "\n",
    "df[\"Year\"] = df[\"Year\"].astype(str).str.zfill(4)\n",
    "df[\"DOY\"] = df[\"DOY\"].astype(str).str.zfill(3)\n",
    "df[\"Hour\"] = df[\"Hour\"].astype(str).str.zfill(2)\n",
    "df[\"Minute\"] = df[\"Minute\"].astype(str).str.zfill(2)\n",
    "df[\"Second\"] = pd.to_numeric(Data[\"Second\"], errors = 'coerce').fillna(0).astype(int).astype(str).str.zfill(2)\n",
    "\n",
    "\n",
    "df[\"Date\"] = df[\"Year\"] + df[\"DOY\"] +df[\"Hour\"] +df[\"Minute\"] +df[\"Second\"]\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"],format = \"%Y%j%H%M%S\", errors = 'coerce')\n",
    "\n",
    "print(\"Datetimes Created\")\n",
    "\n",
    "df.dropna(subset = [\"Date\"], inplace=True)\n",
    "\n",
    "df = df.set_index(\"Date\")\n",
    "df = df.resample(\"1S\").mean().interpolate()\n",
    "\n",
    "p = np.degrees(np.arctan2(Data[\"Y_MSO\"], df[\"X_MSO\"]))+180.\n",
    "dp = np.array(p[1:])- np.array(p[:-1])\n",
    "orbitStartIndex = np.where(dp  < -180)[0]\n",
    "lenp = len(p)\n",
    "\n",
    "del p\n",
    "del dp\n",
    "    \n",
    "loc = np.zeros(df.shape[0]).astype(int)\n",
    "\n",
    "xxx = (crosssort[\"S_Date\"] - df.index[0])\n",
    "xxx = xxx.dt.total_seconds()\n",
    "\n",
    "print(\"Created Location Array\")\n",
    "\n",
    "if np.nanmin(xxx) < 0:\n",
    "    i = np.argmax(np.array(xxx.iloc[np.where(xxx < 0)]))\n",
    "    if crosssort[\"Type\"].iloc[i] == \"BSI\":\n",
    "        loc[:] = 1\n",
    "    \n",
    "    if crosssort[\"Type\"].iloc[i] == \"MPO\":\n",
    "        loc[:] = 1\n",
    "    \n",
    "    if crosssort[\"Type\"].iloc[i] == \"MPI\":\n",
    "        loc[:] = 0\n",
    "    \n",
    "    if crosssort[\"Type\"].iloc[i] == \"BSO\":\n",
    "        loc[:] = 2\n",
    "\n",
    "else:\n",
    "    loc[:] = 2\n",
    "\n",
    "del xxx\n",
    "\n",
    "for i in range(len(crosssort)):\n",
    "    index = np.where((Data.index - crosssort[\"S_Date\"].iloc[i]).total_seconds() == 0)[0]\n",
    "    if len(index) == 1:\n",
    "        index = index[0]\n",
    "        if crosssort[\"Type\"].iloc[i] == \"BSI\":\n",
    "            loc[index:] = -2\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"MPO\":\n",
    "            loc[index:] = -1\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"MPI\":\n",
    "            loc[index:] = -1\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"BSO\":\n",
    "            loc[index:] = -2\n",
    "\n",
    "    index = np.where((Data.index - crosssort[\"E_Date\"].iloc[i]).total_seconds() == 0)[0]\n",
    "    if len(index) == 1:\n",
    "        index = index[0]\n",
    "        if crosssort[\"Type\"].iloc[i] == \"BSI\":\n",
    "            loc[index:] = 1\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"MPO\":\n",
    "            loc[index:] = 1\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"MPI\":\n",
    "            loc[index:] = 0\n",
    "            \n",
    "        if crosssort[\"Type\"].iloc[i] == \"BSO\":\n",
    "            loc[index:] = 2\n",
    "\n",
    "del crosssort\n",
    "print(\"Filled Location Array\")\n",
    "df[\"Location\"] = loc.tolist()\n",
    "\n",
    "orbit = np.zeros(lenp)\n",
    "for i in range(len(orbitStartIndex)):\n",
    "    orbit[orbitStartIndex[i]:] = i\n",
    "\n",
    "del lenp\n",
    "del orbitStartIndex\n",
    "\n",
    "df[\"OrbitNo\"] = orbit.tolist()\n",
    "print(\"Filled Orbit List\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-hamburg",
   "metadata": {},
   "source": [
    "# Labeling Transition Regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proprietary-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopgap_minutes_delta = datetime.timedelta(minutes=10)\n",
    "\n",
    "crossing_list[\"Start_Date\"] -= stopgap_minutes_delta\n",
    "crossing_list[\"End_Date\"] += stopgap_minutes_delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dirty-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial column for transitions (to be changed later)\n",
    "df[\"Transition\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "formed-clinton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f911c37be0104b558b06db72b39dabfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e08168e91534db88f118494910c1408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0:01:53.253003\n",
      "False    14222056\n",
      "True      5913940\n",
      "Name: Transition, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tqdm().pandas()\n",
    "\n",
    "\n",
    "# Labeling date ranges where transitions occurred (with a 10 minute buffer either side)\n",
    "\n",
    "print(\"Labeling Data...\")\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "for idx, row in tqdm(crossing_list.iterrows(), total=len(crossing_list), position = 0, leave = True):\n",
    "    df.loc[row.Start_Date:row.End_Date, \"Transition\"] = True\n",
    "\n",
    "del crossing_list\n",
    "\n",
    "print(\"Elapsed:\", datetime.datetime.now() - start_time)\n",
    "print(df.Transition.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fossil-publication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28a49f7df1346f79e0d959fb14e2435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20135996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-7fe27720e3a1>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.iloc[x-150:x+150][\"Batch\"] = batch\n"
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "x = 0\n",
    "total_data_size = len(df)\n",
    "df[\"Batch\"] = -1\n",
    "\n",
    "pbar = tqdm(total=len(df) , position = 0, leave = True)\n",
    "\n",
    "while x < total_data_size-300:\n",
    "    if df[\"Transition\"].iloc[x-150] == False and df[\"Transition\"].iloc[x+150] == False:\n",
    "        df.iloc[x-150:x+150][\"Batch\"] = batch\n",
    "        batch +=1\n",
    "        x += 300\n",
    "        pbar.update(300)\n",
    "    else: \n",
    "        x += 1\n",
    "        pbar.update(1)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thorough-maximum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Sanity Check\\nbad_sequences = [] \\ndf_1 = df[[\"Location\", \"Batch\"]]\\nfor b in tqdm(range(0,batch), total=batch, position = 0, leave = True): \\n    if len(df_1.loc[df_1[\"Batch\"]== b][\"Location\"].unique()) != 1:\\n        bad_sequences.append(b)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#Sanity Check\n",
    "bad_sequences = [] \n",
    "df_1 = df[[\"Location\", \"Batch\"]]\n",
    "for b in tqdm(range(0,batch), total=batch, position = 0, leave = True): \n",
    "    if len(df_1.loc[df_1[\"Batch\"]== b][\"Location\"].unique()) != 1:\n",
    "        bad_sequences.append(b)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-wilderness",
   "metadata": {},
   "source": [
    "# Creating Testing/Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apparent-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.empty(shape = [int(batch*0.6), 300, 3])\n",
    "test_features = np.empty(shape = [int(batch*0.2), 300, 3])\n",
    "validation_features = np.empty(shape = [int(batch*0.2), 300, 3])\n",
    "\n",
    "train_targets = np.empty(shape = [int(batch*0.6)])\n",
    "test_targets = np.empty(shape = [int(batch*0.2)])\n",
    "validation_targets = np.empty(shape = [int(batch*0.2)])\n",
    "\n",
    "train_len = len(train_targets)\n",
    "test_len = len(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "chief-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df[[\"B_X_MSO\", \"B_Y_MSO\", \"B_Z_MSO\"]]\n",
    "df_targets = df[\"Location\"]\n",
    "df_batches = df[\"Batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exclusive-nomination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb71a3396214491b3c16cb9419af055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_index = 0\n",
    "test_index = 0\n",
    "validation_index = 0\n",
    "\n",
    "pbar = tqdm(total=batch , position = 0, leave = True)\n",
    "b = 1\n",
    "while b < batch -1: \n",
    "    dataset_det = np.random.uniform(0,1)\n",
    "    if dataset_det < 0.6 and train_index < train_len: \n",
    "        train_features[train_index] = df_features.loc[df_batches == b]\n",
    "        train_targets[train_index] = df_targets.loc[df_batches == b].unique()[0]\n",
    "        train_index +=1\n",
    "        b += 1\n",
    "        pbar.update(1)\n",
    "    elif dataset_det < 0.8 and test_index < test_len: \n",
    "        test_features[test_index] = df_features.loc[df_batches == b]\n",
    "        test_targets[test_index] = df_targets.loc[df_batches == b].unique()[0]\n",
    "        test_index +=1\n",
    "        b += 1\n",
    "        pbar.update(1)\n",
    "    elif validation_index < test_len:\n",
    "        validation_features[validation_index] = df_features.loc[df_batches == b]\n",
    "        validation_targets[validation_index] = df_targets.loc[df_batches == b].unique()[0]        \n",
    "        validation_index +=1\n",
    "        b +=1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "negative-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  8.24728985,   4.09839609, -18.5643198 ],\n",
       "        [  8.26497245,   4.12083351, -18.52083222],\n",
       "        [  8.28031515,   4.1444382 , -18.4784707 ],\n",
       "        ...,\n",
       "        [  8.16429714,   7.37711297,  -1.39090568],\n",
       "        [  8.1470292 ,   7.36842333,  -1.34967022],\n",
       "        [  8.13393756,   7.36470537,  -1.27717304]],\n",
       "\n",
       "       [[  8.12016468,   7.34819074,  -1.22537401],\n",
       "        [  8.10206741,   7.34421339,  -1.17470106],\n",
       "        [  8.08334814,   7.33660456,  -1.12027455],\n",
       "        ...,\n",
       "        [  2.49380961,   2.61782889,   6.71998499],\n",
       "        [  2.48302826,   2.59768276,   6.73516006],\n",
       "        [  2.47286891,   2.57892006,   6.75108585]],\n",
       "\n",
       "       [[  0.61050886,  -0.32030608,   0.35052818],\n",
       "        [  0.61376696,  -0.31719338,   0.33170679],\n",
       "        [  0.61743973,  -0.30361852,   0.33696177],\n",
       "        ...,\n",
       "        [  0.637403  ,  -0.18542216,   0.13898869],\n",
       "        [  0.63654404,  -0.18343349,   0.1723953 ],\n",
       "        [  0.64226053,  -0.18460075,   0.17400397]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0.31372549,  -0.5985474 ,  -0.37262052],\n",
       "        [  0.33025295,  -0.62833427,  -0.34377178],\n",
       "        [  0.31123749,  -0.62915568,  -0.34033996],\n",
       "        ...,\n",
       "        [  0.29844204,  -0.66477887,  -0.26650223],\n",
       "        [  0.32127836,  -0.67325235,  -0.28489463],\n",
       "        [  0.31639121,  -0.67221478,  -0.27395571]],\n",
       "\n",
       "       [[  0.31230377,  -0.66897237,  -0.27438469],\n",
       "        [  0.32041941,  -0.65116078,  -0.29384954],\n",
       "        [  0.31271844,  -0.6802127 ,  -0.24655478],\n",
       "        ...,\n",
       "        [  0.3132812 ,  -0.66365484,  -0.25974583],\n",
       "        [  0.29195545,  -0.65829406,  -0.2782991 ],\n",
       "        [  0.30913453,  -0.68522762,  -0.23336372]],\n",
       "\n",
       "       [[  0.3148214 ,  -0.67632182,  -0.24607218],\n",
       "        [  0.28209229,  -0.67126367,  -0.28017588],\n",
       "        [  0.3081571 ,  -0.67416022,  -0.2393694 ],\n",
       "        ...,\n",
       "        [  0.34100468,  -0.6306688 ,  -0.21577564],\n",
       "        [  0.3245957 ,  -0.6395746 ,  -0.24006649],\n",
       "        [  0.3376281 ,  -0.6449786 ,  -0.23304199]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bigger-killing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.46125822e+00,  2.55803900e+00,  6.76272186e+00],\n",
       "        [ 2.44677448e+00,  2.53499633e+00,  6.76626093e+00],\n",
       "        [ 2.43069131e+00,  2.51606070e+00,  6.76636817e+00],\n",
       "        ...,\n",
       "        [-3.17161306e-01, -3.21257187e-01,  4.91999571e+00],\n",
       "        [-3.28238848e-01, -3.10276253e-01,  4.90868143e+00],\n",
       "        [-3.41063918e-01, -2.99252086e-01,  4.88610649e+00]],\n",
       "\n",
       "       [[ 7.33872401e-01, -2.63672128e-01,  1.44243659e-01],\n",
       "        [ 7.36656596e-01, -2.47200726e-01,  1.51750764e-01],\n",
       "        [ 7.34138973e-01, -2.51048377e-01,  1.31535203e-01],\n",
       "        ...,\n",
       "        [ 7.25519815e-01, -3.52297782e-01,  6.53118130e-02],\n",
       "        [ 7.23772288e-01, -3.64705374e-01,  6.27379484e-02],\n",
       "        [ 7.20810379e-01, -3.58566426e-01,  4.55252292e-02]],\n",
       "\n",
       "       [[ 7.43617084e-01, -3.19398210e-01,  1.02686471e-01],\n",
       "        [ 7.41691843e-01, -3.17755393e-01,  1.09013888e-01],\n",
       "        [ 7.34701736e-01, -3.24931910e-01,  1.05099469e-01],\n",
       "        ...,\n",
       "        [ 7.54398436e-01, -3.39284942e-01,  9.27663682e-02],\n",
       "        [ 7.54161483e-01, -3.27914919e-01,  1.00541584e-01],\n",
       "        [ 7.53687578e-01, -3.50568501e-01,  1.04777736e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.71174693e-01, -2.12701569e-02,  5.83462920e-01],\n",
       "        [ 5.69456786e-01, -2.19186373e-02,  5.99388707e-01],\n",
       "        [ 5.66672590e-01, -2.06649086e-02,  6.05555258e-01],\n",
       "        ...,\n",
       "        [ 5.91286061e-01, -2.13868834e-01,  5.17507641e-01],\n",
       "        [ 5.96439784e-01, -2.13436514e-01,  5.08659982e-01],\n",
       "        [ 5.94988449e-01, -1.79369677e-01,  4.93109550e-01]],\n",
       "\n",
       "       [[ 5.86132338e-01, -1.99040249e-01,  5.37347847e-01],\n",
       "        [ 5.77661276e-01, -2.60343262e-01,  5.21904660e-01],\n",
       "        [ 5.93152064e-01, -2.37387056e-01,  6.00353906e-01],\n",
       "        ...,\n",
       "        [ 5.89745868e-01, -3.31762570e-01,  2.09126495e-02],\n",
       "        [ 6.02511700e-01, -3.09757468e-01,  3.14225964e-02],\n",
       "        [ 6.10271903e-01, -3.09065756e-01, -4.82599603e-04]],\n",
       "\n",
       "       [[ 5.93092826e-01, -3.32583978e-01, -1.71590970e-02],\n",
       "        [ 6.01978556e-01, -3.00549047e-01,  3.75891469e-02],\n",
       "        [ 5.99905219e-01, -3.05563962e-01,  5.56598209e-02],\n",
       "        ...,\n",
       "        [ 5.97091405e-01, -4.10661018e-01, -1.26548340e-02],\n",
       "        [ 5.81482140e-01, -4.23457697e-01, -1.15287683e-02],\n",
       "        [ 5.75647177e-01, -4.35260041e-01, -1.24939675e-02]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "italian-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.70819264, -0.38398686,  0.04305861],\n",
       "        [ 0.72084   , -0.35748563,  0.06230897],\n",
       "        [ 0.71974409, -0.36773162,  0.05539171],\n",
       "        ...,\n",
       "        [ 0.74415023, -0.33876616,  0.06016408],\n",
       "        [ 0.73902612, -0.33124379,  0.07319427],\n",
       "        [ 0.74826728, -0.33305953,  0.06729583]],\n",
       "\n",
       "       [[ 0.99742314, -0.16497341,  0.23239852],\n",
       "        [ 0.99843019, -0.17353335,  0.22682181],\n",
       "        [ 0.99715657, -0.1718473 ,  0.21872486],\n",
       "        ...,\n",
       "        [ 1.00882649, -0.28468289,  0.2874685 ],\n",
       "        [ 1.01158107, -0.27145389,  0.2850555 ],\n",
       "        [ 1.01368402, -0.27141066,  0.2678964 ]],\n",
       "\n",
       "       [[ 1.02748652, -0.15386278,  0.24210413],\n",
       "        [ 1.02479119, -0.1433574 ,  0.24623304],\n",
       "        [ 1.02452461, -0.11914746,  0.21245107],\n",
       "        ...,\n",
       "        [ 1.02144423, -0.17491678,  0.1929326 ],\n",
       "        [ 1.01483917, -0.17915352,  0.19539922],\n",
       "        [ 1.01593507, -0.19022092,  0.17941981]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.34316687, -0.20232588, -0.16874899],\n",
       "        [ 0.39799183, -0.10786391, -0.06300606],\n",
       "        [ 0.361353  , -0.04124335, -0.18387045],\n",
       "        ...,\n",
       "        [ 0.27131094, -0.19882409, -0.29020323],\n",
       "        [ 0.26055921, -0.16761057, -0.25904874],\n",
       "        [ 0.25552396, -0.20790281, -0.27379484]],\n",
       "\n",
       "       [[ 0.28952669, -0.16951278, -0.29926538],\n",
       "        [ 0.30531367, -0.12844235, -0.27577886],\n",
       "        [ 0.334992  , -0.11607799, -0.11882675],\n",
       "        ...,\n",
       "        [ 0.26592027, -0.18045048, -0.15067832],\n",
       "        [ 0.28837154, -0.14370326, -0.10579656],\n",
       "        [ 0.31052663, -0.1520038 , -0.08777951]],\n",
       "\n",
       "       [[ 0.45758545, -0.18836194, -0.10858491],\n",
       "        [ 0.46004384, -0.19994812, -0.11829052],\n",
       "        [ 0.44875896, -0.2437854 , -0.08011153],\n",
       "        ...,\n",
       "        [ 0.46969966, -0.2799706 , -0.12048903],\n",
       "        [ 0.47035128, -0.28135403, -0.12059628],\n",
       "        [ 0.47070671, -0.28814146, -0.12504692]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-aurora",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "This section will not be included in the final preprocessing script. It is just here to show how it will be fed into the RNN and to prototype some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "better-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    lstm_layer = keras.layers.RNN(\n",
    "        keras.layers.LSTMCell(units), input_shape=(300, 3)\n",
    "    )\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(output_size),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acknowledged-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "units = 64\n",
    "output_size = 3\n",
    "training_sample_sizes = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wrapped-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3, decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "general-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_3/cp.ckpt\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "august-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = np.unique(train_targets, return_counts = True)[1]/len(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "invalid-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0:weights[0] ,1:weights[1] , 2:weights[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adequate-crest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.1845 - accuracy: 0.8035 - val_loss: 0.4069 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00001: saving model to training_3/cp.ckpt\n",
      "Epoch 2/10\n",
      "437/437 [==============================] - 55s 125ms/step - loss: 0.0445 - accuracy: 0.8850 - val_loss: 0.4016 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00002: saving model to training_3/cp.ckpt\n",
      "Epoch 3/10\n",
      "437/437 [==============================] - 56s 128ms/step - loss: 0.0418 - accuracy: 0.8918 - val_loss: 0.4031 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00003: saving model to training_3/cp.ckpt\n",
      "Epoch 4/10\n",
      "437/437 [==============================] - 55s 127ms/step - loss: 0.0424 - accuracy: 0.8912 - val_loss: 0.4454 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00004: saving model to training_3/cp.ckpt\n",
      "Epoch 5/10\n",
      "437/437 [==============================] - 55s 127ms/step - loss: 0.0402 - accuracy: 0.8901 - val_loss: 0.3910 - val_accuracy: 0.8927\n",
      "\n",
      "Epoch 00005: saving model to training_3/cp.ckpt\n",
      "Epoch 6/10\n",
      "437/437 [==============================] - 55s 127ms/step - loss: 0.0396 - accuracy: 0.8937 - val_loss: 0.4067 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00006: saving model to training_3/cp.ckpt\n",
      "Epoch 7/10\n",
      "437/437 [==============================] - 56s 127ms/step - loss: 0.0388 - accuracy: 0.8942 - val_loss: 0.3690 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00007: saving model to training_3/cp.ckpt\n",
      "Epoch 8/10\n",
      "437/437 [==============================] - 56s 127ms/step - loss: 0.0381 - accuracy: 0.8958 - val_loss: 0.4636 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00008: saving model to training_3/cp.ckpt\n",
      "Epoch 9/10\n",
      "437/437 [==============================] - 56s 127ms/step - loss: 0.0388 - accuracy: 0.8956 - val_loss: 0.3757 - val_accuracy: 0.8986\n",
      "\n",
      "Epoch 00009: saving model to training_3/cp.ckpt\n",
      "Epoch 10/10\n",
      "437/437 [==============================] - 55s 126ms/step - loss: 0.0382 - accuracy: 0.8968 - val_loss: 0.4188 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00010: saving model to training_3/cp.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc6cde66340>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features, train_targets,  \n",
    "        batch_size=batch_size, \n",
    "        validation_data = (validation_features, validation_targets),\n",
    "        epochs=10,\n",
    "        callbacks=[cp_callback],\n",
    "         class_weight = class_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-individual",
   "metadata": {},
   "source": [
    "# Plots and RNN Analysis\n",
    "Again, just here for prototyping reasons. This is a work in progress so if you hit run all, I've put a cell here which will error so it will go: \n",
    "\n",
    "- Data Import \n",
    "- Transition Removal\n",
    "- Sequencing\n",
    "- Test/Train/Val split\n",
    "- Error \n",
    "- Plots and analysis \n",
    "- Outputting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "specified-granny",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-fc9b6cdd1f7a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-fc9b6cdd1f7a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    error here\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "error here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[x-2000:x+2000][\"Location\"].replace(mapping).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#red orange purple  yellow blue \n",
    "mapping = {\"0\": \"#b80000\", \"-1\":\"#5300eb\", \"1\":\"#1273de\", \"-2\":\"#e65100\", \"2\":\"#fccb00\"}\n",
    "\n",
    "## Create custom lines\n",
    "r_line = plt.Line2D((0,1),(0,0), color='#b80000')\n",
    "p_line = plt.Line2D((0,1),(0,0), color='#5300eb')\n",
    "b_line = plt.Line2D((0,1),(0,0), color='#1273de')\n",
    "o_line = plt.Line2D((0,1),(0,0), color='#e65100')\n",
    "y_line = plt.Line2D((0,1),(0,0), color='#fccb00')\n",
    "\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def gen_repeating(s):\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        j = i\n",
    "        while j < len(s) and s[j] == s[i]:\n",
    "            j += 1\n",
    "        yield (s[i], i, j-1)\n",
    "        i = j\n",
    "        \n",
    "\n",
    "\n",
    "## Add px_last lines\n",
    "for color, start, end in gen_repeating(df.iloc[x-2000:x+2000][\"Location\"].replace(mapping)):\n",
    "    if start > 0: # make sure lines connect\n",
    "        start -= 1\n",
    "    idx = df.iloc[x-2000:x+2000].index[start:end+1]\n",
    "    ax.semilogy(np.sqrt(df.loc[idx, 'B_X_MSO']**2+\n",
    "                   df.loc[idx, 'B_Y_MSO']**2+\n",
    "                   df.loc[idx, 'B_Z_MSO']**2),\n",
    "            color=color,\n",
    "            label='',\n",
    "            linewidth = 2)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "\n",
    "## Create legend from custom artist/label lists\n",
    "ax.legend(\n",
    "    handles + [r_line, \n",
    "               p_line, \n",
    "               b_line, \n",
    "               o_line,\n",
    "               y_line \n",
    "               ],\n",
    "    labels + [\n",
    "        'Magnetosphere',\n",
    "        \"Magnetopause\",\n",
    "        'Magnetosheath',\n",
    "        \"Bow Shock\",       \n",
    "        'Solar Wind'\n",
    "    ],\n",
    "    fontsize=20,\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1.2, 1.0175)\n",
    ")\n",
    "\n",
    "#Formating x-axis dates\n",
    "ax.xaxis_date()\n",
    "fig.autofmt_xdate()\n",
    "formatter =  DateFormatter('%Y-%h-%d %H:%M')\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.ylabel(\"|B| (nT)\")\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-representation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-gauge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-decision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-raleigh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-navigation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-advantage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-calibration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-rating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-chart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputing Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare paths to the data directory and files of interest\n",
    "data_directory = Path(\"../data\")\n",
    "data_files = [\n",
    "    \"test_list.csv\",\n",
    "    \"train_list.csv\",\n",
    "    \"validation_list.csv\"\n",
    "]\n",
    "\n",
    "print(\"Parsing data files...\")\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for data_file in data_files:\n",
    "    # Iterate over each file,\n",
    "    print(f\"Parsing {data_file}...\")\n",
    "    start_time_file = datetime.datetime.now()\n",
    "\n",
    "    # We define the data types to speed up reading and ensure they're saved properly in the Parquet\n",
    "    df = pd.read_csv(\n",
    "        data_directory / data_file,\n",
    "        sep=',', skipinitialspace=True, index_col=\"Date\",\n",
    "        dtype={\n",
    "          'X_MSO': np.float64, 'Y_MSO': np.float64, 'Z_MSO': np.float64,\n",
    "          'B_X_MSO': np.float64, 'B_Y_MSO': np.float64, 'B_Z_MSO': np.float64,\n",
    "          'Location': 'category',\n",
    "          'OrbitNo': np.int32\n",
    "        },\n",
    "        parse_dates=[\"Date\"], infer_datetime_format=True\n",
    "    )\n",
    "\n",
    "    # Now we save to Parquet, and return the space saving (a bit unnecessary...)\n",
    "    output_file = data_file.split('.')[0]+'.parquet'\n",
    "    df.to_parquet(data_directory / output_file)\n",
    "    print(\n",
    "        f\"Elapsed {datetime.datetime.now() - start_time_file}, size from \"\n",
    "        f\"{(data_directory/data_file).stat().st_size / 1024**3:.2f}GB to \"\n",
    "        f\"{(data_directory / output_file).stat().st_size / 1024**3:.2f}GB\"\n",
    "    )\n",
    "\n",
    "print('Total elapsed:', datetime.datetime.now() - start_time)\n",
    "\n",
    "Data.to_csv(dir_path+\"/MESSENGER_data.csv\")\n",
    "del Data\n",
    "del orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-cause",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-provincial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-event",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-tackle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-journalism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-brunei",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-configuration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-senate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-trash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-dakota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-tiger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-chemistry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-smile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
